{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# LIMPANDO"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binary_classification = pd.read_csv(\"2019-05-28_portuguese_hate_speech_binary_classification.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text  hatespeech_comb  \\\n",
       "0  @__andrea__b \\r\\nO cara vive em outro mundo\\r\\...                1   \n",
       "1  @_carmeloneto Estes incompetentes não cuidam n...                0   \n",
       "2  @_carmeloneto \\r\\nOs 'cumpanhero' quebraram to...                0   \n",
       "\n",
       "   hatespeech_G1 annotator_G1  hatespeech_G2 annotator_G2  hatespeech_G3  \\\n",
       "0              1            A            1.0            V              0   \n",
       "1              1            D            0.0            V              0   \n",
       "2              1            A            0.0            B              0   \n",
       "\n",
       "  annotator_G3  \n",
       "0            E  \n",
       "1            C  \n",
       "2            E  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>hatespeech_comb</th>\n      <th>hatespeech_G1</th>\n      <th>annotator_G1</th>\n      <th>hatespeech_G2</th>\n      <th>annotator_G2</th>\n      <th>hatespeech_G3</th>\n      <th>annotator_G3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@__andrea__b \\r\\nO cara vive em outro mundo\\r\\...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>A</td>\n      <td>1.0</td>\n      <td>V</td>\n      <td>0</td>\n      <td>E</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@_carmeloneto Estes incompetentes não cuidam n...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>D</td>\n      <td>0.0</td>\n      <td>V</td>\n      <td>0</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@_carmeloneto \\r\\nOs 'cumpanhero' quebraram to...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>A</td>\n      <td>0.0</td>\n      <td>B</td>\n      <td>0</td>\n      <td>E</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df_binary_classification.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hate = df_binary_classification[['text', 'hatespeech_comb']]\n",
    "df_hate.rename(columns = \n",
    "{\n",
    "    'hatespeech_comb':'hate',\n",
    "}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text  hate\n",
       "0  @__andrea__b \\r\\nO cara vive em outro mundo\\r\\...     1\n",
       "1  @_carmeloneto Estes incompetentes não cuidam n...     0\n",
       "2  @_carmeloneto \\r\\nOs 'cumpanhero' quebraram to...     0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@__andrea__b \\r\\nO cara vive em outro mundo\\r\\...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@_carmeloneto Estes incompetentes não cuidam n...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@_carmeloneto \\r\\nOs 'cumpanhero' quebraram to...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df_hate.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "from unidecode import unidecode\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "\n",
    "#    word_tokens = word_tokenize(string)\n",
    "    \n",
    "    string = re.sub(r\"@[A-Za-z0-9]+\", ' ', string)\n",
    "    string = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', string)\n",
    "    # Remove as imagens\n",
    "    string = re.sub(r'pic.twitter.com/\\S+', ' ', string)\n",
    "    string = re.sub(r'(\\<.*?\\>)', ' ', string, flags=re.UNICODE)\n",
    "    \n",
    "    string = re.sub(r\"[^a-zA-Z.!?']\", ' ', string)\n",
    "    string = re.sub(r\" +\", ' ', string)\n",
    "    \n",
    "    word_tokens = word_tokenize(string.lower())\n",
    "    \n",
    "    stop_words = set(stopwords.words('portuguese') + list(punctuation))\n",
    "    palavras_sem_stopwords = [palavra for palavra in word_tokens if palavra not in stop_words]\n",
    "    \n",
    "    string = ' '.join(palavras_sem_stopwords)\n",
    "\n",
    "    \n",
    "    return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hate['clean_text'] = df_hate['text'].apply(clean_str)\n",
    "\n",
    "df_hate=df_hate.reindex(columns= ['text', 'clean_text', 'hate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text  \\\n",
       "0  @__andrea__b \\r\\nO cara vive em outro mundo\\r\\...   \n",
       "1  @_carmeloneto Estes incompetentes não cuidam n...   \n",
       "2  @_carmeloneto \\r\\nOs 'cumpanhero' quebraram to...   \n",
       "\n",
       "                                          clean_text  hate  \n",
       "0  andrea b cara vive outro mundo n mundo real re...     1  \n",
       "1  carmeloneto incompetentes n cuidam povo brasil...     0  \n",
       "2     carmeloneto 'cumpanhero quebraram todas regras     0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>clean_text</th>\n      <th>hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@__andrea__b \\r\\nO cara vive em outro mundo\\r\\...</td>\n      <td>andrea b cara vive outro mundo n mundo real re...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@_carmeloneto Estes incompetentes não cuidam n...</td>\n      <td>carmeloneto incompetentes n cuidam povo brasil...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@_carmeloneto \\r\\nOs 'cumpanhero' quebraram to...</td>\n      <td>carmeloneto 'cumpanhero quebraram todas regras</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "df_hate.head(3)"
   ]
  },
  {
   "source": [
    "# BERT"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Flatten, Dropout\n",
    "from keras.models import Sequential, Model\n",
    "from keras import metrics\n",
    "import keras.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sbert_model = SentenceTransformer('neuralmind/bert-large-portuguese-cased')\n",
    "#sbert_model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "#embedder = SentenceTransformer('bert-large-nli-stsb-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df_hate[\"clean_text\"].values\n",
    "labels = df_hate['hate'].values\n",
    "sentence_embeddings = sbert_model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_long_sentence = max([len(s) for s in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainDeepLearningModel(maiorTamanho, tamanhoVocabulario, numDimensions, matriz_embedding = None):\n",
    "\n",
    "    print(\"Maior tamanho:\", maiorTamanho)\n",
    "    print(\"Tamanho vocabulário:\", tamanhoVocabulario)\n",
    "    print(\"Numero dimensões:\", numDimensions)\n",
    "    \n",
    "    deep_inputs = Input(shape=(numDimensions,))\n",
    "    embedding = Embedding(tamanhoVocabulario, numDimensions, input_length=maiorTamanho, weights=[matriz_embedding], trainable=False)(deep_inputs) # line A\n",
    "    flatten = Flatten()(embedding)\n",
    "    hidden1 = Dense(20, activation='relu')(flatten)\n",
    "\n",
    "    h4 = Dense(10, activation='sigmoid')(hidden1)\n",
    "    hidden3 = Dense(2, activation='softmax')(h4)\n",
    "    model = Model(inputs=deep_inputs, outputs=hidden3)\n",
    "\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[metrics.mean_squared_error, metrics.mean_absolute_error])\n",
    "    model.summary();\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Maior tamanho: 130\n",
      "Tamanho vocabulário: 5670\n",
      "Numero dimensões: 512\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 512)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 512, 512)          2903040   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 262144)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 20)                5242900   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 8,146,172\n",
      "Trainable params: 5,243,132\n",
      "Non-trainable params: 2,903,040\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = TrainDeepLearningModel(\n",
    "    length_long_sentence, \n",
    "    sentence_embeddings.shape[0], \n",
    "    sentence_embeddings.shape[1], \n",
    "    sentence_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/2\n",
      "178/178 [==============================] - 12s 65ms/step - loss: 0.2503 - mean_squared_error: 0.2503 - mean_absolute_error: 0.5000\n",
      "Epoch 2/2\n",
      "178/178 [==============================] - 11s 64ms/step - loss: 0.2500 - mean_squared_error: 0.2500 - mean_absolute_error: 0.5000\n"
     ]
    }
   ],
   "source": [
    "model.fit(sentence_embeddings, labels, epochs=2, verbose=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   positive  negative\n",
       "0  0.479165  0.520835"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>positive</th>\n      <th>negative</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.479165</td>\n      <td>0.520835</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "teste = sbert_model.encode(['eu odeio pretos']);\n",
    "resultado = pd.DataFrame(model.predict(teste), columns=[\"positive\", 'negative']);\n",
    "resultado"
   ]
  },
  {
   "source": [
    "### Não entendi direito o q fiz ai em cima... Uma opção para explorar é essa da [internet]('https://towardsdatascience.com/sentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671')"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Outra opção é [essa](https://github.com/thomas-ferraz/FakeNews-BERTimbau/blob/main/Modelo_BERTimbau.ipynb) que usa BERTimbau para analisar Fake News.\n",
    "\n",
    "Vamos testar ela abaixo."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separar treino e test do treinamento\n",
    "\n",
    "#aleatorizar e separar a 0\n",
    "df_train=df.loc[df['class'] == 0].sample(frac=0.7,random_state=12) #random state is a seed value\n",
    "df_val=df.loc[df['class'] == 0].drop(df_train.index)\n",
    "\n",
    "#aleatorizar e separar a 1\n",
    "df_train2=df.loc[df['class'] == 1].sample(frac=0.7,random_state=12) #random state is a seed value\n",
    "df_val2=df.loc[df['class'] == 1].drop(df_train2.index)\n",
    "\n",
    "#Juntar\n",
    "df_train = pd.concat([df_train, df_train2])\n",
    "df_val = pd.concat([df_val, df_val2])\n",
    "\n",
    "#aleatorizar\n",
    "from sklearn.utils import shuffle\n",
    "df_train = shuffle(df_train, random_state=12)\n",
    "df_val = shuffle(df_val, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = df_val['text'].to_numpy()\n",
    "X_train = df_train['text'].to_numpy()\n",
    "Y_val = df_val['class'].to_numpy()\n",
    "Y_train = df_train['class'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, TFAutoModel\n",
    "\n",
    "Tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-large-portuguese-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence, max_len=3000):\n",
    "    tokens = Tokenizer.encode_plus(sentence, max_length=max_len,\n",
    "                                   truncation=True, padding='max_length',\n",
    "                                   add_special_tokens=True, return_attention_mask=True,\n",
    "                                   return_token_type_ids=False, return_tensors='tf')\n",
    "    return tokens['input_ids'], tokens['attention_mask']\n",
    "\n",
    "def token_encode(X, max_len):\n",
    "  input_ids = []\n",
    "  masks = []\n",
    "  for text in X:\n",
    "    text_input_id, text_mask = tokenize(text, max_len=max_len)\n",
    "    input_ids.append(tf.reshape(text_input_id, [max_len]))\n",
    "    masks.append(tf.reshape(text_mask, [max_len]))\n",
    "  return np.array(input_ids), np.array(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(max_len=512, type_model='OnlyBERT'):\n",
    "  input_ids = tf.keras.layers.Input(shape=(max_len,), name='input_ids', dtype=tf.int32)\n",
    "  mask = tf.keras.layers.Input(shape=(max_len,), name='attention_mask', dtype=tf.int32)\n",
    "  \n",
    "  # By December 12th Hugging Face's Transformer Library only had the PyTorch version of BERTimbau so we needed to\n",
    "  # convert the PyTorch model to TensorFlow using the AutoConfig class and adding from_pt (from pytorch) equal to True\n",
    "  config = AutoConfig.from_pretrained('neuralmind/bert-large-portuguese-cased')\n",
    "  BERTimbau = TFAutoModel.from_pretrained('neuralmind/bert-large-portuguese-cased', from_pt=True, config=config)\n",
    "\n",
    "  last_bert_hidden_layer = BERTimbau(input_ids, attention_mask=mask)[0]\n",
    "\n",
    "  if type_model == 'OnlyBERT':\n",
    "    net = tf.keras.layers.Dense(64, activation='relu')(last_bert_hidden_layer)\n",
    "    net = tf.keras.layers.Dropout(0.2)(net)\n",
    "    net = tf.keras.layers.Flatten()(net)\n",
    "    net = tf.keras.layers.Dense(32, activation='relu')(net)\n",
    "    net = tf.keras.layers.Dropout(0.2)(net)\n",
    "    out = tf.keras.layers.Dense(1, activation='sigmoid')(net)\n",
    "\n",
    "  elif type_model == 'BERT_LSTM':\n",
    "    net = tf.keras.layers.Dense(64, activation='relu')(last_bert_hidden_layer)\n",
    "    net = tf.keras.layers.Dropout(0.2)(net)\n",
    "    net = tf.keras.layers.Dense(32, activation='relu')(net)\n",
    "    net = tf.keras.layers.Dropout(0.2)(net)\n",
    "    net = tf.keras.layers.Flatten()(net)\n",
    "    out = tf.keras.layers.Dense(1, activation='sigmoid')(net)\n",
    "\n",
    "  model = tf.keras.models.Model(inputs=[input_ids, mask], outputs=out)\n",
    "  model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100\n",
    "\n",
    "X_train = token_encode(df_train['text'].to_numpy(), max_len)\n",
    "X_val = token_encode(df_val['text'].to_numpy(), max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = build_model(max_len=max_len)\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history_3 = model3.fit(\n",
    "    X_train, Y_train, \n",
    "    validation_data=(X_val, Y_val),\n",
    "    epochs=10,\n",
    "    batch_size=8,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = model3.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_ajust(y):\n",
    "  y_hat = []\n",
    "  for x in y:\n",
    "    if x >= 0.5:\n",
    "      y_hat.append(1)\n",
    "    else:\n",
    "      y_hat.append(0)\n",
    "  return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "Y_test = y_ajust(Y_test)\n",
    "conf_matrix = metrics.confusion_matrix(Y_val, Y_test)\n",
    "print(conf_matrix)\n",
    "print(metrics.classification_report(Y_val, Y_test, digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.ConfusionMatrixDisplay(conf_matrix, display_labels=[0,1]).plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(train_history_3, \"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(train_history_3, \"loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}