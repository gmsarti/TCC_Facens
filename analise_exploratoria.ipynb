{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# LIMPANDO"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binary_classification = pd.read_csv(\"2019-05-28_portuguese_hate_speech_binary_classification.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text  hatespeech_comb  \\\n",
       "0  @__andrea__b \\r\\nO cara vive em outro mundo\\r\\...                1   \n",
       "1  @_carmeloneto Estes incompetentes não cuidam n...                0   \n",
       "2  @_carmeloneto \\r\\nOs 'cumpanhero' quebraram to...                0   \n",
       "\n",
       "   hatespeech_G1 annotator_G1  hatespeech_G2 annotator_G2  hatespeech_G3  \\\n",
       "0              1            A            1.0            V              0   \n",
       "1              1            D            0.0            V              0   \n",
       "2              1            A            0.0            B              0   \n",
       "\n",
       "  annotator_G3  \n",
       "0            E  \n",
       "1            C  \n",
       "2            E  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>hatespeech_comb</th>\n      <th>hatespeech_G1</th>\n      <th>annotator_G1</th>\n      <th>hatespeech_G2</th>\n      <th>annotator_G2</th>\n      <th>hatespeech_G3</th>\n      <th>annotator_G3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@__andrea__b \\r\\nO cara vive em outro mundo\\r\\...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>A</td>\n      <td>1.0</td>\n      <td>V</td>\n      <td>0</td>\n      <td>E</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@_carmeloneto Estes incompetentes não cuidam n...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>D</td>\n      <td>0.0</td>\n      <td>V</td>\n      <td>0</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@_carmeloneto \\r\\nOs 'cumpanhero' quebraram to...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>A</td>\n      <td>0.0</td>\n      <td>B</td>\n      <td>0</td>\n      <td>E</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "df_binary_classification.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hate = df_binary_classification[['text', 'hatespeech_comb']]\n",
    "df_hate.rename(columns = \n",
    "{\n",
    "    'hatespeech_comb':'hate',\n",
    "}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text  hate\n",
       "0  @__andrea__b \\r\\nO cara vive em outro mundo\\r\\...     1\n",
       "1  @_carmeloneto Estes incompetentes não cuidam n...     0\n",
       "2  @_carmeloneto \\r\\nOs 'cumpanhero' quebraram to...     0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@__andrea__b \\r\\nO cara vive em outro mundo\\r\\...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@_carmeloneto Estes incompetentes não cuidam n...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@_carmeloneto \\r\\nOs 'cumpanhero' quebraram to...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "df_hate.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "from unidecode import unidecode\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "\n",
    "#    word_tokens = word_tokenize(string)\n",
    "    \n",
    "    string = re.sub(r\"@[A-Za-z0-9]+\", ' ', string)\n",
    "    string = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', string)\n",
    "    # Remove as imagens\n",
    "    string = re.sub(r'pic.twitter.com/\\S+', ' ', string)\n",
    "    string = re.sub(r'(\\<.*?\\>)', ' ', string, flags=re.UNICODE)\n",
    "    \n",
    "    string = re.sub(r\"[^a-zA-Z.!?']\", ' ', string)\n",
    "    string = re.sub(r\" +\", ' ', string)\n",
    "    \n",
    "    word_tokens = word_tokenize(string.lower())\n",
    "    \n",
    "    stop_words = set(stopwords.words('portuguese') + list(punctuation))\n",
    "    palavras_sem_stopwords = [palavra for palavra in word_tokens if palavra not in stop_words]\n",
    "    \n",
    "    string = ' '.join(palavras_sem_stopwords)\n",
    "\n",
    "    \n",
    "    return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hate['clean_text'] = df_hate['text'].apply(clean_str)\n",
    "\n",
    "df_hate=df_hate.reindex(columns= ['text', 'clean_text', 'hate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text  \\\n",
       "0  @__andrea__b \\r\\nO cara vive em outro mundo\\r\\...   \n",
       "1  @_carmeloneto Estes incompetentes não cuidam n...   \n",
       "2  @_carmeloneto \\r\\nOs 'cumpanhero' quebraram to...   \n",
       "\n",
       "                                          clean_text  hate  \n",
       "0  andrea b cara vive outro mundo n mundo real re...     1  \n",
       "1  carmeloneto incompetentes n cuidam povo brasil...     0  \n",
       "2     carmeloneto 'cumpanhero quebraram todas regras     0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>clean_text</th>\n      <th>hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@__andrea__b \\r\\nO cara vive em outro mundo\\r\\...</td>\n      <td>andrea b cara vive outro mundo n mundo real re...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@_carmeloneto Estes incompetentes não cuidam n...</td>\n      <td>carmeloneto incompetentes n cuidam povo brasil...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@_carmeloneto \\r\\nOs 'cumpanhero' quebraram to...</td>\n      <td>carmeloneto 'cumpanhero quebraram todas regras</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "df_hate.head(3)"
   ]
  },
  {
   "source": [
    "# BERT"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Flatten, Dropout\n",
    "from keras.models import Sequential, Model\n",
    "from keras import metrics\n",
    "import keras.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sbert_model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "#embedder = SentenceTransformer('bert-large-nli-stsb-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df_hate[\"clean_text\"].values\n",
    "labels = df_hate['hate'].values\n",
    "sentence_embeddings = sbert_model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_long_sentence = max([len(s) for s in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainDeepLearningModel(maiorTamanho, tamanhoVocabulario, numDimensions, matriz_embedding = None):\n",
    "\n",
    "    print(\"Maior tamanho:\", maiorTamanho)\n",
    "    print(\"Tamanho vocabulário:\", tamanhoVocabulario)\n",
    "    print(\"Numero dimensões:\", numDimensions)\n",
    "    \n",
    "    deep_inputs = Input(shape=(numDimensions,))\n",
    "    embedding = Embedding(tamanhoVocabulario, numDimensions, input_length=maiorTamanho, weights=[matriz_embedding], trainable=False)(deep_inputs) # line A\n",
    "    flatten = Flatten()(embedding)\n",
    "    hidden1 = Dense(20, activation='relu')(flatten)\n",
    "\n",
    "    h4 = Dense(10, activation='sigmoid')(hidden1)\n",
    "    hidden3 = Dense(2, activation='softmax')(h4)\n",
    "    model = Model(inputs=deep_inputs, outputs=hidden3)\n",
    "\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[metrics.mean_squared_error, metrics.mean_absolute_error])\n",
    "    model.summary();\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Maior tamanho: 130\n",
      "Tamanho vocabulário: 5670\n",
      "Numero dimensões: 512\n",
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 512)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 512, 512)          2903040   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 262144)            0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 20)                5242900   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 8,146,172\n",
      "Trainable params: 5,243,132\n",
      "Non-trainable params: 2,903,040\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = TrainDeepLearningModel(\n",
    "    length_long_sentence, \n",
    "    sentence_embeddings.shape[0], \n",
    "    sentence_embeddings.shape[1], \n",
    "    sentence_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/2\n",
      "178/178 [==============================] - 10s 55ms/step - loss: 0.2501 - mean_squared_error: 0.2501 - mean_absolute_error: 0.5000\n",
      "Epoch 2/2\n",
      "178/178 [==============================] - 11s 60ms/step - loss: 0.2500 - mean_squared_error: 0.2500 - mean_absolute_error: 0.5000\n"
     ]
    }
   ],
   "source": [
    "model.fit(sentence_embeddings, labels, epochs=2, verbose=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   positive  negative\n",
       "0       0.5       0.5"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>positive</th>\n      <th>negative</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.5</td>\n      <td>0.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "teste = sbert_model.encode(['eu odeio pretos']);\n",
    "resultado = pd.DataFrame(model.predict(teste), columns=[\"positive\", 'negative']);\n",
    "resultado"
   ]
  },
  {
   "source": [
    "### Não entendi direito o q fiz ai em cima... vou tentar outro modelo da [internet]('https://towardsdatascience.com/sentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671')"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 433/433 [00:00<00:00, 32.3kB/s]\n",
      "Downloading: 100%|██████████| 536M/536M [03:10<00:00, 2.81MB/s]\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertForSequenceClassification: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['dropout_37', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 409kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5670, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "df_hate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] O sistema não pode encontrar o caminho especificado: 'aclImdb/train'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-4399c68a6a03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'training'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m123\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\preprocessing\\text_dataset.py\u001b[0m in \u001b[0;36mtext_dataset_from_directory\u001b[1;34m(directory, labels, label_mode, class_names, batch_size, max_length, shuffle, seed, validation_split, subset, follow_links)\u001b[0m\n\u001b[0;32m    144\u001b[0m       \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m       \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m       follow_links=follow_links)\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mlabel_mode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'binary'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\preprocessing\\dataset_utils.py\u001b[0m in \u001b[0;36mindex_directory\u001b[1;34m(directory, labels, formats, class_names, shuffle, seed, follow_links)\u001b[0m\n\u001b[0;32m     63\u001b[0m   \"\"\"\n\u001b[0;32m     64\u001b[0m   \u001b[0minferred_class_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m   \u001b[1;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m       \u001b[0minferred_class_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] O sistema não pode encontrar o caminho especificado: 'aclImdb/train'"
     ]
    }
   ],
   "source": [
    "# We create a training dataset and a validation \n",
    "# dataset from our \"aclImdb/train\" directory with a 80/20 split.\n",
    "train = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/train', \n",
    "    batch_size=2500, \n",
    "    validation_split=0.2, \n",
    "    subset='training', \n",
    "    seed=123\n",
    ")\n",
    "\n",
    "test = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/train', \n",
    "    batch_size=3170, \n",
    "    validation_split=0.2,\n",
    "    subset='validation', \n",
    "    seed=123\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-0184cfb222cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m   \u001b[0mtrain_feat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m   \u001b[0mtrain_lab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_feat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_lab\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "for i in train.take(1):\n",
    "  train_feat = i[0].numpy()\n",
    "  train_lab = i[1].numpy()\n",
    "\n",
    "train = pd.DataFrame([train_feat, train_lab]).T\n",
    "train.columns = ['clean_text', 'hate']\n",
    "train['clean_text'] = train['clean_text'].str.decode(\"utf-8\")\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}